{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark;\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spark session \n",
    "- Spark session is the `entry point for Spark` library.\n",
    "- `pyspark.sql` can be considered as an interface between datasources (e.g Hive, csv files, databases etc)\n",
    "- `pyspark.sql` provided streamlined SQL-like interface to interact with DataFrame obtained from datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"lin_reg_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------+-------------+--------+\n",
      "|Freq_Hz|AoA_Deg|Chord_m|V_inf_mps|displ_thick_m|sound_db|\n",
      "+-------+-------+-------+---------+-------------+--------+\n",
      "|    800|    0.0| 0.3048|     71.3|    0.0026634|   126.2|\n",
      "|   1000|    0.0| 0.3048|     71.3|    0.0026634|   125.2|\n",
      "|   1250|    0.0| 0.3048|     71.3|    0.0026634|  125.95|\n",
      "|   1600|    0.0| 0.3048|     71.3|    0.0026634|  127.59|\n",
      "|   2000|    0.0| 0.3048|     71.3|    0.0026634|  127.46|\n",
      "|   2500|    0.0| 0.3048|     71.3|    0.0026634|  125.57|\n",
      "|   3150|    0.0| 0.3048|     71.3|    0.0026634|   125.2|\n",
      "|   4000|    0.0| 0.3048|     71.3|    0.0026634|  123.06|\n",
      "|   5000|    0.0| 0.3048|     71.3|    0.0026634|   121.3|\n",
      "|   6300|    0.0| 0.3048|     71.3|    0.0026634|  119.54|\n",
      "|   8000|    0.0| 0.3048|     71.3|    0.0026634|  117.15|\n",
      "|  10000|    0.0| 0.3048|     71.3|    0.0026634|  115.39|\n",
      "|  12500|    0.0| 0.3048|     71.3|    0.0026634|  112.24|\n",
      "|  16000|    0.0| 0.3048|     71.3|    0.0026634|  108.72|\n",
      "|    500|    0.0| 0.3048|     55.5|    0.0028308|  126.42|\n",
      "|    630|    0.0| 0.3048|     55.5|    0.0028308|   127.7|\n",
      "|    800|    0.0| 0.3048|     55.5|    0.0028308|  128.09|\n",
      "|   1000|    0.0| 0.3048|     55.5|    0.0028308|  126.97|\n",
      "|   1250|    0.0| 0.3048|     55.5|    0.0028308|  126.09|\n",
      "|   1600|    0.0| 0.3048|     55.5|    0.0028308|  126.99|\n",
      "+-------+-------+-------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Cols = [\"Freq_Hz\", \"AoA_Deg\", \"Chord_m\", \"V_inf_mps\", \"displ_thick_m\"]\n",
    "Y_Col = \"sound_db\"\n",
    "df = sparkSession.read.csv('./airfoil_self_noise.csv', header=True, inferSchema=True)\n",
    "#df_X = df.select(X_Cols)\n",
    "#df_Y = df.select(Y_Col)\n",
    "df.show()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Freq_Hz: integer (nullable = true)\n",
      " |-- AoA_Deg: double (nullable = true)\n",
      " |-- Chord_m: double (nullable = true)\n",
      " |-- V_inf_mps: double (nullable = true)\n",
      " |-- displ_thick_m: double (nullable = true)\n",
      " |-- sound_db: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our schema is full of string (if we don't infer schema while reading from datasource), so we need to transform it into `numbers` in order to do some operations.\n",
    "\n",
    "- This casting can be done using `Column.cast()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def cast_col():    \n",
    "    casted_freq = df['Freq_Hz'].cast(DoubleType())\n",
    "    return df.withColumn('_Freq_Hz', casted_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorAssembler` assembles all the feature into one. \n",
    "\n",
    "- This is required because Apis in spark expects only 2 columns, i.e feature column and output column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   unscaled_features|sound_db|\n",
      "+--------------------+--------+\n",
      "|[800.0,0.0,0.3048...|   126.2|\n",
      "|[1000.0,0.0,0.304...|   125.2|\n",
      "|[1250.0,0.0,0.304...|  125.95|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler;\n",
    "\n",
    "\"\"\"\n",
    "def has_column(colName=\"\", df):\n",
    "    return df.columns.contains(colName)\n",
    "\"\"\"\n",
    "\n",
    "def assemble_features(df):\n",
    "    _col_to_add = \"feature\"\n",
    "    df = df.drop(_col_to_add)\n",
    "    assembler = VectorAssembler(inputCols=X_Cols, outputCol=\"unscaled_features\")\n",
    "    df = assembler.transform(df)\n",
    "    return df.select([\"unscaled_features\", Y_Col])\n",
    "\n",
    "clean_df = assemble_features(df)\n",
    "clean_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to bring data on same/similar scale before fitting it into the algorithm. We have 2 options here:\n",
    "\n",
    "- Standardization of data (`pyspark.ml.feature.StandardScaler`)\n",
    "- Normalization of data (`pyspark.ml.feature.Normalizer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|sound_db|\n",
      "+--------------------+--------+\n",
      "|[0.25376096453669...|   126.2|\n",
      "|[0.31720120567086...|   125.2|\n",
      "|[0.39650150708858...|  125.95|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def std_scale_df(df, _input_col=\"unscaled_features\"):\n",
    "    freq_scaler = StandardScaler(inputCol=_input_col, outputCol=\"features\")\n",
    "    return freq_scaler.fit(df).transform(df);\n",
    "\n",
    "scale_df = std_scale_df(clean_df);\n",
    "scale_df = scale_df.select([\"features\", Y_Col])\n",
    "scale_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`randomSplit()` generates the splitted dataframe based on split configuration passed in argumet.\n",
    "\n",
    "**Note**: splitted dataframe is not guaranteed to have exact number(proportion) of element as specified in split confguration. \n",
    "Every element in dataframe is guaranteed to have equally-likeliness to be in either of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, cv_df, test_df = scale_df.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndesc_train = train_df.describe()\\ndesc_cv = cv_df.describe()\\ndesc_test = test_df.describe()\\nprint(desc_train[desc_train.summary == 'count'].show())\\nprint(desc_cv[desc_cv.summary == 'count'].show())\\nprint(desc_test[desc_test.summary == 'count'].show())\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "desc_train = train_df.describe()\n",
    "desc_cv = cv_df.describe()\n",
    "desc_test = test_df.describe()\n",
    "print(desc_train[desc_train.summary == 'count'].show())\n",
    "print(desc_cv[desc_cv.summary == 'count'].show())\n",
    "print(desc_test[desc_test.summary == 'count'].show())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|sound_db|\n",
      "+--------------------+--------+\n",
      "|[0.06344024113417...|  117.19|\n",
      "|[0.06344024113417...|  118.13|\n",
      "|[0.06344024113417...|  128.68|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.754475122299981"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression;\n",
    "\n",
    "estimator = LinearRegression(labelCol=Y_Col)\n",
    "model = estimator.fit(train_df)\n",
    "\n",
    "model.summary\n",
    "model.evaluate(train_df).rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation \n",
    "\n",
    "- R^2 - This is a typical guideline(but not thumb rule), `Higher the R2 better the model`.\n",
    "- R2 - (Variance explained by Model **/** Total variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.925295283614273\n",
      "0.4653855397200154\n"
     ]
    }
   ],
   "source": [
    "cv_result = model.evaluate(cv_df)\n",
    "print(cv_result.rootMeanSquaredError)\n",
    "print(cv_result.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not able to nicely fit into the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48183407493301744\n"
     ]
    }
   ],
   "source": [
    "test_result = model.evaluate(test_df)\n",
    "print(test_result.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist a model\n",
    "\n",
    "- When all looks good, save a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlwriter = model.write();\n",
    "mlwriter.overwrite().save('model_lin_reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model gets saved into current directory.\n",
    "- Model is saved in `parquet` structure.\n",
    "\n",
    "- We can also store model in `PMML` structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LinearRegressionModel.load('model_lin_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|           _features|            features|       prediction|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|[2500.0,4.0,0.228...|[0.0,0.0,0.0,0.0,...|131.6272735005637|\n",
      "+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_sample = df.sample(False, 0.01).limit(1)\n",
    "_assembler = VectorAssembler(inputCols=X_Cols, outputCol=\"_features\")\n",
    "tuned_sample = _assembler.transform(user_sample).select('_features')\n",
    "tuned_sample = std_scale_df(tuned_sample, _input_col=\"_features\")\n",
    "_user_prediction = loaded_model.transform(tuned_sample)\n",
    "_user_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s 21\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    a = \"s\";\n",
    "    b = 21\n",
    "    return a, b\n",
    "\n",
    "x, y = foo()\n",
    "print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
